{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, jaccard_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import uniform, loguniform\n",
    "from scipy.stats import randint\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Binary Relevance Approach and Classifier Chains Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary relevance approach, a multi-classification problem is broken down into multiple binary classification sup-problems by creating a classifier for each label. On the other hand, in the classifier chains approach, a sequence of binary classifiers are created but, unlike the binary relevance approach, the classifiers are not independent as predictions from the previous classifiers are used as inputs for the next classifier. This means that the binary relevance approach is simpler and less computationally expensive than the classifier chains approach. However, the classifier chains approach can capture label correlations better than the binary relevance approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Multi-Label Classifier Using the Binary Relevance Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1405\n",
      "Hamming Loss: 0.1926\n",
      "Jaccard Score: 0.5087\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('yeast.csv')\n",
    "\n",
    "data.drop(0)\n",
    "X = data.iloc[:, :103].values \n",
    "y = data.iloc[:, 103:].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,               # For reproducibility\n",
    "    early_stopping=True,           # Enable early stopping\n",
    "    validation_fraction=0.1        # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create and train the binary relevance classifier\n",
    "binary_relevance = BinaryRelevance(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True]     # Both X and y should be dense matrices\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "binary_relevance.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = binary_relevance.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming = hamming_loss(y_test_dense, y_pred_dense)\n",
    "jaccard_score = jaccard_score(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2314\n",
      "Hamming Loss: 0.2050\n",
      "Jaccard Score: 0.5205\n"
     ]
    }
   ],
   "source": [
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,              # For reproducibility\n",
    "    early_stopping=True,          # Enable early stopping\n",
    "    validation_fraction=0.1       # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create and train the classifier chain\n",
    "classifier_chain = ClassifierChain(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True],    # Both X and y should be dense matrices\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "classifier_chain.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier_chain.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "from sklearn.metrics import jaccard_score as js_metric\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming = hamming_loss(y_test_dense, y_pred_dense)\n",
    "js = js_metric(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Jaccard Score: {js:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Hyperparameters to Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hyperparameters chosen to optimize are the number of hidden layers and the number of neurons in each layer. The hidden layer size can impact the model's ability to model the data, which is especially in a multi-label dataset as the model needs to be able to learn the relationships between the inoput features and the multiple labels. Finding the optimal number of hidden layers and neurons is important as it ensures that the model as too few neurons can lead to underfitting, while too many neurons can lead to overfitting.\n",
    "\n",
    "Another hyperparameter that is selected for optimization is the learning rate. The learning rate is important as it determines how the model learns its weights. If the learning rate is too high it may overshoot the optimal values and fail to converge. On the other hand, if the learning rate is too low, the model may take a long time to converge or may get stuck in a local minimum. Therefore, performing hyperparameter optimization on the learning rate can help to find the optimal learning rate for the model such that learning is balanced between all the labels.\n",
    "\n",
    "Alpha is another hyperparameter that is selected for optimization. Alpha is the L2 regularization parameter that is used to prevent overfitting. This is especially important for datasets with a large number of features as they are more prone to overfitting and in datasets with multiple labels as to prevent the model from overfitting to one label. \n",
    "\n",
    "Lastly, the batch size is selected for optimization. Batch size is the number of samples that are used to update the model's weights. Small batch sizes can lead to better generalization and but can be computationally expensive. On the other hand, large batch sizes can lead to faster training times but can lead to poor generalization. Finding the optimal batch size is important in a multi-label dataset as the batch size needs to be large enough to contain enough label combinations while also being small enough to prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an HPO Technique Supported By the `scikit-learn` Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen HPO technique is random search. Random search is chosen as it is more computational efficient, especially in the case for large datsets since it randomly samples the search space instead of exploring every possible parameter combination. Additionally, randoms search allows for continuous hyperparameters to be optimized. This is useful when optimizing hyperparameters such as learning rates and alpha as it allows us to find more accurate values for these hyperparameters. Lastly, random search is better able to explore the search space and find important hyperparameters when compared to grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Suitable Value of ùêæ for Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of K chosen for ùêæ-foldcross-validation is 5. 5 was chosen as it ensures we have enough data for training and validation. While choosing a larger value of K could be more benifificial since each fold will train on more data it would also be more computationally expensive. Additonally, the dataset is large and therefore each fold will have enough training samples and will contain suffienct examples for all of the 14 labels despite the smaller value of K."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
