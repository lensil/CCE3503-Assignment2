{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, jaccard_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import uniform, loguniform\n",
    "from scipy.stats import randint\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Binary Relevance Approach and Classifier Chains Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary relevance approach, a multi-classification problem is broken down into multiple binary classification sup-problems by creating a classifier for each label. On the other hand, in the classifier chains approach, a sequence of binary classifiers are created but, unlike the binary relevance approach, the classifiers are not independent as predictions from the previous classifiers are used as inputs for the next classifier. This means that the binary relevance approach is simpler and less computationally expensive than the classifier chains approach. However, the classifier chains approach can capture label correlations better than the binary relevance approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Multi-Label Classifier Using the Binary Relevance Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('yeast.csv')\n",
    "\n",
    "data.drop(0)\n",
    "X = data.iloc[:, :103].values \n",
    "y = data.iloc[:, 103:].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,               # For reproducibility\n",
    "    early_stopping=True,           # Enable early stopping\n",
    "    validation_fraction=0.1        # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create and train the binary relevance classifier\n",
    "binary_relevance = BinaryRelevance(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True]     # Both X and y should be dense matrices\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "binary_relevance.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = binary_relevance.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "print(\"Prediction type:\", type(y_pred))\n",
    "print(\"Prediction shape:\", y_pred.shape if hasattr(y_pred, 'shape') else \"no shape\")\n",
    "print(\"Dense prediction type:\", type(y_pred_dense))\n",
    "print(\"Dense prediction shape:\", y_pred_dense.shape)\n",
    "print(\"Test type:\", type(y_test_dense))\n",
    "print(\"Test shape:\", y_test_dense.shape)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming = hamming_loss(y_test_dense, y_pred_dense)\n",
    "jaccard_score = jaccard_score(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,              # For reproducibility\n",
    "    early_stopping=True,          # Enable early stopping\n",
    "    validation_fraction=0.1       # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create and train the classifier chain\n",
    "classifier_chain = ClassifierChain(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True],    # Both X and y should be dense matrices\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "classifier_chain.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier_chain.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "from sklearn.metrics import jaccard_score as js_metric\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming = hamming_loss(y_test_dense, y_pred_dense)\n",
    "js = js_metric(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Jaccard Score: {js:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
