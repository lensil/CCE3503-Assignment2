{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, jaccard_score, f1_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference Between Binary Relevance Approach and Classifier Chains Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the binary relevance approach, a multi-classification problem is broken down into multiple binary classification sup-problems by creating a classifier for each label. On the other hand, in the classifier chains approach, a sequence of binary classifiers are created but, unlike the binary relevance approach, the classifiers are not independent as predictions from the previous classifiers are used as inputs for the next classifier. This means that the binary relevance approach is simpler and less computationally expensive than the classifier chains approach. However, the classifier chains approach can capture label correlations better than the binary relevance approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Multi-Label Classifier Using the Binary Relevance Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment, the ratio chosen for splitting the data into training and testing sets is 0.8. This is because it ensures there is enough data for the model to train on while also ensuring that there is enough data for the model to test on and verify its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1405\n",
      "Hamming Loss: 0.1926\n",
      "Jaccard Score: 0.5087\n",
      "F1 Score: 0.6210\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('yeast.csv')\n",
    "\n",
    "data.drop(0)\n",
    "X = data.iloc[:, :103].values \n",
    "y = data.iloc[:, 103:].values\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,               # For reproducibility\n",
    "    early_stopping=True,           # Enable early stopping\n",
    "    validation_fraction=0.1        # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create the binary relevance classifier\n",
    "binary_relevance = BinaryRelevance(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True]     \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "binary_relevance.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = binary_relevance.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_binary = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming_binary = hamming_loss(y_test_dense, y_pred_dense)\n",
    "jaccard_score_binary = jaccard_score(y_test_dense, y_pred_dense, average='samples')\n",
    "f1_score_binary = f1_score(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy_binary:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_binary:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard_score_binary:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_binary:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.2314\n",
      "Hamming Loss: 0.2050\n",
      "Jaccard Score: 0.5205\n",
      "F1 Score: 0.6136\n"
     ]
    }
   ],
   "source": [
    "# Create a base classifier\n",
    "base_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers\n",
    "    activation='relu',             # ReLU activation function\n",
    "    solver='adam',                 # Adam optimizer\n",
    "    max_iter=300,                  # Maximum iterations\n",
    "    random_state=42,              # For reproducibility\n",
    "    early_stopping=True,          # Enable early stopping\n",
    "    validation_fraction=0.1       # Use 10% of training data for validation\n",
    ")\n",
    "\n",
    "# Create the classifier chain\n",
    "classifier_chain = ClassifierChain(\n",
    "    classifier=base_classifier,\n",
    "    require_dense=[True, True],    \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "classifier_chain.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = classifier_chain.predict(X_test)\n",
    "\n",
    "# Convert sparse matrix predictions to dense array for evaluation\n",
    "y_pred_dense = y_pred.toarray()\n",
    "y_test_dense = y_test\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_chains = accuracy_score(y_test_dense, y_pred_dense)\n",
    "hamming_chains = hamming_loss(y_test_dense, y_pred_dense)\n",
    "jaccard_score_chains = jaccard_score(y_test_dense, y_pred_dense, average='samples')\n",
    "f1_score_chains = f1_score(y_test_dense, y_pred_dense, average='samples')\n",
    "\n",
    "print(f\"Subset Accuracy: {accuracy_chains:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_chains:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard_score_chains:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_chains:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adapted algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Hyperparameters to Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hyperparameters chosen to optimize are the number of hidden layers and the number of neurons in each layer. The hidden layer size can impact the model's ability to model the data, which is especially in a multi-label dataset as the model needs to be able to learn the relationships between the inoput features and the multiple labels. Finding the optimal number of hidden layers and neurons is important as it ensures that the model as too few neurons can lead to underfitting, while too many neurons can lead to overfitting.\n",
    "\n",
    "Another hyperparameter that is selected for optimization is the learning rate. The learning rate is important as it determines how the model learns its weights. If the learning rate is too high it may overshoot the optimal values and fail to converge. On the other hand, if the learning rate is too low, the model may take a long time to converge or may get stuck in a local minimum. Therefore, performing hyperparameter optimization on the learning rate can help to find the optimal learning rate for the model such that learning is balanced between all the labels.\n",
    "\n",
    "Alpha is another hyperparameter that is selected for optimization. Alpha is the L2 regularization parameter that is used to prevent overfitting. This is especially important for datasets with a large number of features as they are more prone to overfitting and in datasets with multiple labels as to prevent the model from overfitting to one label. \n",
    "\n",
    "Lastly, the batch size is selected for optimization. Batch size is the number of samples that are used to update the model's weights. Small batch sizes can lead to better generalization and but can be computationally expensive. On the other hand, large batch sizes can lead to faster training times but can lead to poor generalization. Finding the optimal batch size is important in a multi-label dataset as the batch size needs to be large enough to contain enough label combinations while also being small enough to prevent overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting an HPO Technique Supported By the `scikit-learn` Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen HPO technique is random search. Random search is chosen as it is more computational efficient, especially in the case for large datsets since it randomly samples the search space instead of exploring every possible parameter combination. Additionally, randoms search allows for continuous hyperparameters to be optimized. This is useful when optimizing hyperparameters such as learning rates and alpha as it allows us to find more accurate values for these hyperparameters. Lastly, random search is better able to explore the search space and find important hyperparameters when compared to grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Suitable Value of ùêæ for Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of K chosen for ùêæ-foldcross-validation is 5. 5 was chosen as it ensures we have enough data for training and validation. While choosing a larger value of K could be more benifificial since each fold will train on more data it would also be more computationally expensive. Additonally, the dataset is large and therefore each fold will have enough training samples and will contain suffienct examples for all of the 14 labels despite the smaller value of K."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a Neural Network Multi-Label Classifier Using HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters found:\n",
      "{'alpha': np.float64(0.006187670675880952), 'batch_size': 32, 'hidden_layer_sizes': (100, 50), 'learning_rate_init': np.float64(0.004895834359555106)}\n",
      "\n",
      "Best cross-validation score: 0.6233\n",
      "\n",
      "Test Set Performance:\n",
      "Subset Accuracy: 0.1839\n",
      "Hamming Loss: 0.1942\n",
      "Jaccard Score: 0.5323\n",
      "F1 Score: 0.6367\n",
      "\n",
      "Output layer activation function:\n",
      "logistic\n"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions \n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50,25), (100,50), (100,50,25)],\n",
    "    'learning_rate_init': loguniform(1e-4, 1e-1),\n",
    "    'alpha': loguniform(1e-4, 1e-2),\n",
    "    'batch_size': [32, 64, 128, 256, 'auto']\n",
    "}\n",
    "\n",
    "# Create base neural network \n",
    "base_nn = MLPClassifier(\n",
    "    activation='relu',           # ReLU for hidden layers\n",
    "    solver='adam',              \n",
    "    max_iter=300,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Configure RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=base_nn,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=50,\n",
    "    cv=5,                      # 5-fold cross-validation\n",
    "    scoring='f1_samples',      # F1 score for multi-label classification\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"\\nBest cross-validation score: {random_search.best_score_:.4f}\")\n",
    "\n",
    "# Get the best model\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy_hpo = accuracy_score(y_test, y_pred)\n",
    "hamming_hpo = hamming_loss(y_test, y_pred)\n",
    "jaccard_score_hpo = jaccard_score(y_test, y_pred, average='samples')\n",
    "f1_score_hpo = f1_score(y_test, y_pred, average='samples')\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"Subset Accuracy: {accuracy_hpo:.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_hpo:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard_score_hpo:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_hpo:.4f}\")\n",
    "\n",
    "# Print information about the neural network's last layer\n",
    "print(\"\\nOutput layer activation function:\")\n",
    "print(best_model.out_activation_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network is adapted so that its outputs are suitable for multi-label classification since `MLPClassifier` automatically adapts for multi-label classification by using a sigmoid function at the output layer. This can be seen in the code above when printinting the output layer activation function, which is a logistic (sigmoid) function. The sigmoid function is important as it allows each neuron to make independent predictions between 0 and 1, which is important for multi-label classification as each label can be predicted independently of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Suitable Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance metrics chosen are accuracy, hamming loss, jaccard score and f1 score. \n",
    "\n",
    "Accuracy is chosen as it allows us to see how many predicitions were able to correctly predict all 14 output labels. However, it is important to note that due to this it can yield low scores and hence it is not a suitable evaluation metric on its own. \n",
    "\n",
    "The next metric chosen is hamming loss. Hamming loss is chosen as it gives us a better picture of how accurate our model is. It measures the fraction of labels that are incorrectly predicted. This is important as it allows us to see how many labels were predicted incorrectly and how many were predicted correctly. It allows us to see how our model performs on a label by label basis.\n",
    "\n",
    "The third metric chosen is the jaccard score. The jaccard score is chosen as it measures the similarity between the predicted labels and the true labels. This means that it is able to consider both partial and complete matches between the predicted and the functional classes.\n",
    "\n",
    "Lastly, the f1 score is chosen as it is the harmonic mean of precision and recall. This helps us to view how many of the predicted labels were correct and how many of the actual labels were predicted and hence it shows how the model is able to balance between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the accuracy of the three models, the classifier chains model had the best accuracy at 0.2314, followed the the adapted algorithm model at 0.1839 and lastly the binary relevance model 0.1405. This is expected as the classifier chains model is able to capture the relationships between the labels better since each classifier learn from the predictions of previous classifiers. On the other hand, the binary relevance models low score is also expected since it treats labels as being independent of each other. It is also expected that the accuracy scores are low as the accuracy requires all labels to be predicted correctly for a prediction which can be difficult in a multi-classification problem.\n",
    "\n",
    "Looking at the hamming loss, we can observe that the binary relevance model had the best hamming loss at 0.1926, followed closedly by the adapted algorithm at 0.1942 and lastly the classifier chains model at 0.2050. All three aproaches had similar scores, indicating that they performed similarly in predicting the individual labels. These scores also highlight the importance of using multiple evaluation metrics as despite the low accuracy scores, they all were able to predict around 80% of the inidividual labels correctly. It can also be observed that while the classifier chains achieved the best accuracy score, it had the worst hamming loss score. Conversely, the binary relevance model had the best hamming loss score but the worst accuracy score. This indicates a possible trade-off between the two metrics.\n",
    "\n",
    "The adapted algoirthm model had the best jaccard score of 0.5323, followed by the classifier chains at 0.5205 and lastly the binary relevance model with a score of 0.5087. This suggests that all three being able to achieve a reasonable overlap between the predicted and true labels. The higher score of the adapted algorithm model indicates that hyperparameter optimization allowed the model to perform better in predicting the labels.\n",
    "\n",
    "Finally, considering the f1 score, the adapted algorithm model had the best score at 0.6367, followed by the binary relevance model at 0.6210 and lastly the classifier chains model at 0.6136. The adapted algorithm model having the best score is expected as it used the f1 score during hyperparameter optimization to find the optimal hyperparameters for the model and hence was able to mantain the best balance between precision and recall.\n",
    "\n",
    "Overall, the adapted algorithm performed the best as it was able to have a good balance of scores and mantain consistency across all the evaluation metrics. This is expected as the model was optimized using hyperparameter optimization and cross-validation and hence was able to find the optimal hyperparameters for the model. However, all three models were able to achieve reasonable scores across all the evaluation metrics, indicating that they were able to perform reasonably well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main challenges encountered was trying to achieve good scores across all the evaluation metrics. This was likely due to the dataset containing multiple labels. This was particularly challenging during hyperparameter optimization as it was important to choose an evalaution metric that would be able to optimize hyperparameters while not sacrificing the performance of the model in other evaluation metrics. Ultimately, the f1 score was used as it is able to balance between precision and recall. \n",
    "\n",
    "Each approach had its own strengths and weakness when it came to predicting the labels. The binary relevance model was able to predict the individual labels the best but failed to capture the relationship between the labels which lead to the model have the lowest accuracy score. The classifier chains model was able to capture the relationships between the labels the best and hence had the best accuracy score but then had the lowest hamming loss score, indicating a trade off between global and local predication accuracy. The adapted algorithm model was able to achieve a good balance between the two which resulted in it having the best jaccard and f1 scores and also had a good balance between all scores. However, hyperparameter optimization can be computationally expensive and time consuming, especially in large datasets.\n",
    "\n",
    "Based on these results, one potential improved could be to combine the binary relevance and classifier chains approaches. This could be done by using the binary relevance model to predict the individual labels and then using the classifier chains model to predict the relationships between the labels. Another possible improvement could be to use a different HPO technique such as Bayesian optimization. Bayesian optimization is able to model the search space and hence is might be able to find better hyperparameters than random search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://scikit-learn.org/stable/\n",
    "\n",
    "- http://scikit.ml\n",
    "\n",
    "- J.D. Kelleher, B. Mac Namee and A. D‚ÄôArcy, ‚ÄúFundamentals of Machine Learning for Predictive Data Analytics‚Äù, MIT Press.\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Multi-label_classification\n",
    "\n",
    "- https://www.geeksforgeeks.org/comparing-randomized-search-and-grid-search-for-hyperparameter-estimation-in-scikit-learn/\n",
    "\n",
    "- https://medium.com/biased-algorithms/evaluation-metrics-for-classification-models-b995f9980716\n",
    "\n",
    "- https://www.kdnuggets.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-explained   \n",
    "\n",
    "- AI tools were used to aid in clarification of concepts, help with debugging (e.g. aiding in understanding what was causing an error) and further explain how the scikit-learn methods used for this assignment work (e.g. what parameters they take and what they do)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
